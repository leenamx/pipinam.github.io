---
layout:     post                    # 使用的布局（不需要改）
title:      阿南带你推导GAN           # 标题 
subtitle:   干就完了!                # 副标题
date:       2023-03-27              # 时间
author:     Nam                     # 作者
header-img: img/post-bg-MLE & MAP-highway.jpg    # 这篇文章标题背景图片
mathjax: true                       # 使用mathjax
catalog: true                       # 是否归档
tags:                               # 标签
    - 阿南面试直通车
---

# 阿南带你学习GAN

## 1、前言

因为忙于工作和生活的缘故，大约已经2年多没有更新过博客了。最近在学习过往经典的深度学习算法时有感，体会到了一种简单明了的美，想借此机会记录下这生活中学习成长的美好。

今天介绍的主题是生成对抗网络，英文缩写的发音很有趣，中文读做“干”！英文全称是 **Gennerative Adversarial Networks (GAN)**


## 2、简介

在之前的博客中我介绍了 **最大似然估计(MLE)**，我们现在知道可以通过最大似然估计的方法来获得一个模型：$P_{G}(x;\theta)$ 用于生成想要的数据。之所以可以用它来生成图像是因为最大似然估计可以得到参数 $\theta$ 能够让模型 $P_{G}(x;\theta)$ 去拟合真实数据的分布情况。

上述的最大似然估计的方法虽然在大部分时候可行，但还是存在一些约束，即 $P_{G}(x;\theta)$ 不能够过于复杂。例如如果 $P_{G}(x;\theta)$ 服从正态分布，那么通过最大似然估计的方法就可以估计出 $P_{G}(x;\theta)$，但如果 $P_{G}(x;\theta)$ 是一个非常复杂的分布的话，使用最大似然估计就很难获得拟合比较理想的模型。

但我们希望 $P_{G}(x;\theta)$ 能够拟合任何复杂的分布，于是今天的主角 **GAN** 便闪亮登场！

## 3、**GAN**

在GAN中有两个主要的组成部分，既生成器 **Generator** 和判别器 **Discriminator**。其中生成器的作用是仿造真实数据集去人为地生成一个能够以假乱真的数据，而判别器的作用是去辨别接收到的输入是来源于真实数据还是人工生成的。它们两互相对抗成长，共同进步，相爱相杀（闻到了恋爱的腐臭味）。

因为使用**极大似然估计**很难计算拟合复杂的分布，而神经网络理论上可以拟合任意的分布，所以GAN的方法就是使用一个神经网络来完成拟合真实的数据分布，这个神经网络就是**生成器G**。

很有意思的是，我们喂给生成器的输入是**随机噪声**，这个噪声可能来自正态分布、均匀分布或随机分布。其实我刚开始学习的时候很好奇，为什么这里输入的是随机噪声呢？后来通过查阅资料和咨询后恍然大悟，因为我们可以将输入给生成器的随机噪声可以看作是输入数据在特征空间中的一个随机采样点，GAN的生成器就是将接收到的低维随机噪声向量映射到高维空间，生成近似于真实数据的虚假数据。

这个相爱相杀的过程通常会在一段时间后达到一个平衡，即满足零和博弈中的 **纳什均衡（Nash Equilibrium）**，根据接下来的公式推导，在纳什均衡状态下，生成器和判别器分别处于以下状态：

1、**生成器：** 此时生成器所生成的数据十分相像于真实数据，可以以假乱真了。生成器已经学习到了真实数据的分布。

2、**判别器：** 此时判别器无法区分真实数据和假数据，由于生成器已经生成了逼真的假数据，判别器已经无法正常判别，对于任何接受到的输入都输出0.5的概率（等于抛硬币瞎猜）。

### 3.1 生成器 (Generator)
因此对于生成器而言，其目标是最小化生成器分布 $P_{G}$ 与真实数据分布 $P_{data}$ 之间的距离，因此我们可以写出其目标函数如下：

$$
G^{*} = \arg \min_{G} Div(P_{G}, P_{data}) \tag{1}
$$

但问题就是我们无法准确的知道真实分布 $P_{data}$ 和生成分布 $P_{G}$ 的具体分布情况，所以我们依旧使用采样的方式来解决这个问题，即从数据集中抽取一个样本，将抽出的样本的分布看成是$P_{data}$ 和 $P_{G}$ 的分布。

其实就是运用了 **大数定律** 的思想，知道两个分布之后就可以通过训练生成器来最小化两个分布之间的差异。

### 3.2 判别器 (Discriminator)

在上面小节中我们知道生成器可以最小化生成分布与真实分布之间的距离，但它们两之间的距离 $Div(P_{G}, P_{data})$ 该如何定义呢？

这个时候判别器闪亮登场！！！我们采用判别器来定义这两个分布之间的距离。

我们从真实数据分布 $P_{data}$ 中抽样得到真实数据 $x_{true}$，以及从噪声 $Z$ 中抽样得到 $z$ 并输入生成器 $G$ 得到生成数据 $G(z)$  

然后我们用真实数据 $x$ 和生成数据 $G(z)$ 来训练判别器，目的就是让判别器可以分辨出哪些是真实数据，并**给真实数据 $x$ 打高分**，即最大化：

$$
\log{D(x)}  \tag{2}
$$

对于生成分布 $P_{G}$ 中抽样得到的样本 $G(z)$ 打低分，即最大化

$$
\log{(1-D(G(z)))} \tag{3}
$$

### 3.3 价值函数

根据判别器的目标将公式2、3结合得到判别器的目标函数：

$$
\begin{aligned}
D^{*} &= \arg \max_{D}V(G,D) \\ 
      &= E_{x \sim p_{data}}[\log{D(x)}] + E_{z \sim p_{Z}}[\log{(1-D(G(z)))}]
\end{aligned}
\tag{4}
$$

在结合公式1以及生成器的概念，我们可以得到生成器的目标函数：

$$
\begin{aligned}
G^{*}& = \arg \min_{G} V(G,D^{*}) \\
     & = \arg\min_{G} \max_{D} V(G,D) \\
     & = E_{x \sim p_{data}}[\log{D(x)}] + E_{z \sim p_{Z}}[\log{(1-D(G(z)))}] \\
\end{aligned}
\tag{5}
$$


至此，我们得到了GAN的价值函数：
$$
V(G,D) = E_{x \sim p_{data}}[\log{D(x)}] + E_{z \sim p_{Z}}[\log{(1-D(G(z)))}]
\tag{6}
$$

### 3.4 数学推导
通过前面的讨论，我们已经掌握了生成器和判别器的目标函数，现在我们进一步对目标函数何时取得最优解的情况进行推导。

因为在训练生成器之前，需要定义两个分布间的距离，所以先推导求解极大值的部分，即判别器的目标函数。

$$
\begin{aligned}
D^{*} &= \arg \max_{D}V(G,D) \\ 
\end{aligned}
\tag{7}
$$

我们先将GAN的价值函数转变成积分的形式：

$$
V(G,D) = \int_{x} p_{data}(x) \cdot \log{D(x)} dx + \int_{G(z)} p_z(z) \cdot \log{(1-D(G(z)))} dz \tag{8}
$$

此处假设 $x=G(z)$ 采用积分换元后得到：

$$
\begin{aligned}
V(G,D) &= \int_{x} p_{data}(x) \cdot \log{D(x)} dx + \int_{x} p_G(x) \cdot \log{(1-D(x))} dx \\
       &= \int_{x} p_{data}(x) \cdot \log{D(x)} + p_G(x) \cdot \log{(1-D(x))} dx
\end{aligned}
\tag{9}
$$

此处的 $p_G(x)$ 代表了由噪音 $z$ 生成的 $x$ 的分布。由于换元这个方式要求生成器 $G$ 必须可逆，但对于深度神经网络来说基本上是不可能的，GAN原作者忽略了这个可逆条件，所以最优解的推导不够完美，这里我们就不再展开描述啦。

随后我们对价值函数 $V(G,D)$ 进行最大化求解，即对 $D$ 进行优化令价值函数 $V$ 取得最大：

$$
\begin{aligned}

& \frac{\partial{}}{\partial{D(x)}} {(p_{data}(x) \cdot \log{D(x)} + p_G(x) \cdot \log{(1-D(x))})} = 0 \\

& \Rightarrow \frac{p_{data}(x)}{D(x)} + \frac{p_{G}(x)}{1-D(x)} = 0 \\

& \Rightarrow D(x) = \frac{p_{data}(x)}{p_{data}(x)+p_{g}(x)}

\end{aligned}
\tag{10}
$$



经过推导得到判别器 $D$ 的最优解 $D^{\*}$ 为:

$$
D^{*} = \frac{p_{data}(x)}{p_{data}(x)+p_{g}(x)}
\tag{11}
$$

得到判别器的最优解 $D^{\*}$ 后，我们将得到的解带回价值函数 $V(G,D)$ ，由于 $D$ 已经固定，转换为求使得价值函数最小值的生成器解 $G^{\*}$ 得到：

$$
\begin{aligned}

V(G,D^{*}) 
        
        &= \int_{x} { \left[ p_{data}(x) \cdot \log{D^{*}(x)} + p_G(x) \cdot \log{(1-D^{*}(x))} \right] dx} \\

        &=  \int_{x} \left[ p_{data}(x) \cdot \log{\frac{p_{data}(x)}{p_{data}(x)+p_{g}(x)}} + p_G(x) \cdot \log{(1-\frac{p_{data}(x)}{p_{data}(x)+p_{g}(x)})} \right] dx \\

        &=  \int_{x} \left[ p_{data}(x) \cdot \log{\frac{p_{data}(x)}{p_{data}(x)+p_{g}(x)}} + p_G(x) \cdot \log{(\frac{p_{g}(x)}{p_{data}(x)+p_{g}(x)})} \right] dx \\

        &=  \int_{x} \left[ p_{data}(x) \cdot \log{\frac{\frac{p_{data}(x)}{2}}{\frac{p_{data}(x)+p_{g}(x)}{2}}} + p_G(x) \cdot \log{(\frac{\frac{p_{g}(x)}{2}}{\frac{p_{data}(x)+p_{g}(x)}{2}})} \right] dx \\

        &=  \int_{x} \left[ p_{data}(x) \cdot \log{\frac{\frac{p_{data}(x)}{2}}{\frac{p_{data}(x)+p_{g}(x)}{2}}} + p_G(x) \cdot \log{(\frac{\frac{p_{g}(x)}{2}}{\frac{p_{data}(x)+p_{g}(x)}{2}})} \right] dx \\
        
\end{aligned}
\tag{12}
$$



接下来介绍一下  $KL$ 散度(Kullback-Leibler divergence) 和 $JS$ 散度(Jensen-Shannon divergence)

$KL$ 散度又称 $KL$ 距离，相对熵。KL散度是描述两个概率分布 $A$ 和 $B$ 之间差异的一种方法。用于衡量两个分布的偏离程度，如果两个分布完全匹配，那么 $KL(A\mid \mid B)=0$，否则其取值应该在0～$\infty$之间。$KL$ 散度越小，两个分布之间的相似度越高。

并且简单介绍一下 $KL$ 散度的两个基本性质：

1、非负性
$$
KL(A \mid\mid B) >= 0
\tag{13}
$$

2、不对称性
$$
{KL(A \mid \mid B)} \ne {KL(B \mid \mid A)}
\tag{14}
$$

接下来给出离散KL散度的计算公式：

$$
KL(A \mid \mid B) = \sum{A(x)\log{\frac{A(x)}{B(x)}}}
\tag{15}
$$

以及连续概率分布的 $KL$ 散度计算公式：

$$
KL(A \mid \mid B) = \int{A(x) \log{\frac{A(x)}{B(x)}}}
\tag{16}
$$

然后我们来介绍 $KL$ 散度的兄弟 $JS$ 散度！$JS$ 散度也称 $JS$ 距离，是$KL$ 散度的一种变形。接下来给出 $JS$ 散度的数学定义如下：

$$
JS(A \mid \mid B) = \frac{1}{2} KL(A \mid \mid \frac{A+B}{2}) + \frac{1}{2} KL(B \mid \mid \frac{A+B}{2})
\tag{17}
$$


$JS$ 散度不同于 $KL$ 散度的两个方面如下：

1、值域范围

$JS$ 散度的值域范围是[0,1]，相同为0，相反则为1。相较于$KL$ 散度来说，$JS$ 散度对于相似度的判别更为准确。

2、对称性

$JS$ 散度是对称的，即 $JS(A \mid \mid B) = JS(B \mid \mid A)$

**（未完待续，持续更新）**

