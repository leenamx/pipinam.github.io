---
layout:     post                    # 使用的布局（不需要改）
title:      阿南带你理解MLE和MAP     # 标题 
subtitle:   七分靠打拼，三分天注定    # 副标题
date:       2020-03-20              # 时间
author:     Nam                     # 作者
header-img: img/post-bg-MLE & MAP-highway.jpg    # 这篇文章标题背景图片
mathjax: true                       # 使用mathjax
catalog: true                       # 是否归档
tags:                               # 标签
    - 阿南面试直通车
---


# **前言**

最近一直在准备找工作的事情，所以博客跟新的速度就有所放缓。

正所谓金三银四嘛！所以最近阿南来带大家复习一下机器学习中重要的知识点，也助大家在找工作的时候顺利通关！

这次的博客篇幅会很短，不过好东西嘛，在精而不在长！今天要给大家介绍的是统计学中的重要知识： **极大似然估计** 和 **最大后验估计**。

统计学分为 **频率学派** 与 **贝叶斯学派**。

**频率学派:** 从 "自然" 角度出发，试图直接为 "事件" 本身建模，即事件  $A$ 在独立重复试验中发生的频率趋于极限，那么这个极限就是该事件的概率 $p$。***频率学派的代表是最大似然估计。***

**贝叶斯学派:** 并不从试图刻画 "事件" 本身，而从「观察者」角度出发。贝叶斯学派并不试图说「事件本身是随机的」，或者「世界的本体带有某种随机性」，这套理论根本不言说关于「世界本体」的东西，而只是从 **"观察者知识不完备"** 这一出发点开始，构造一套在贝叶斯概率论的框架下可以对不确定知识做出推断的方法。***贝叶斯学派的代表是最大后验概率估计。***


<br/>

# **极大似然估计**

**极大似然估计 (Maximum Likelihood Estimation, MLE)** ：***<u>用来估计概率模型的参数的一种方法</u>***。

极大似然估计的应用场景：**模型已知，参数未知**

我们的目的是：**通过找到一个参数 $\theta$ 能够使得整个样本集 $X$ 出现的概率 $P(X\mid \theta)$ 最大**，即

$$
\begin{aligned}
   \arg \max_{\theta} P(X|\theta) & = P(x_1 |\theta) P(x_2|\theta) \cdots P(x_N|\theta) \\
                                  & = \prod_{i=1}^N P(x_i|\theta)
\end{aligned} \tag{1}
$$

得到似然函数 $L(\theta)$

$$
L(\theta) = \prod_{i=1}^N P(x_i|\theta) \tag{2}
$$

由于似然函数中存在连乘符号，难以进行优化，因此我们将似然函数 $L(\theta)$ 取对数，得到对数似然函数 $\log L(\theta)$

$$
\log L(\theta) = \sum_{i=1}^N P(x_i|\theta) \tag{3}
$$

有的小伙伴可能会有疑惑，可以这样随随便便的就对原始似然函数求对数嘛？

阿南的回答是：**可以！**

因为对数函数是一个单调递增函数，取对数之后并不会改变原函数的单调性，因此我们可以放心大胆地对原始似然函数取对数后进行优化！

得到对数似然函数之后，让其对参数 $\theta$ 求偏导并令其结果为零，最后解出 $\theta$。

$$
\frac{\partial \log L(\theta)}{\partial \  \theta} = 0 \tag{4}
$$

此时得到的模型参数 $\theta$ 就是能够使得样本集 $X$ 出现概率最大的模型参数。

<br/>

# **最大后验估计**

最大后验估计 (Maximum a posterior estimation, MAP)：***<u>利用贝叶斯公式求解出模型参数，最大后验概率估计可以看作是正则化的最大似然估计。 </u>***

最大后验估计的应用场景：**模型已知，参数未知。但此时有额外的关于模型参数的先验信息 $P(\theta)$。**

首先，列出大名鼎鼎的 **贝叶斯公式**

$$
P(\theta|X) = \frac{P(X|\theta) P(\theta)}{P(X)} = \frac{P(X|\theta) P(\theta)}{\sum_{\theta}P(X|\theta) P(\theta)} \tag{5}
$$

随后我们通过找到求解能使参数 $\theta$ 的后验概率 $P(\theta \mid X)$ 最大的参数值就得到模型参数 $\theta$ 的最大后验估计 $\theta_{MAP}$

$$
\theta_{MAP} = \arg \max_{\theta} P(\theta|X) \tag{6}
$$

观察式 $(5)$ 中右端的分母我们可以发现，数据集 $X$ 的先验分布 $P(X)$ 我们可以通过分析数据获得。并且我们也并不关心数据集的分布，我们只关心模型的参数 $\theta$，因此在这里我们可以忽略分母中的先验分布 $P(X)$。

因此，式 $(5)$ 可以改写为

$$
\begin{aligned}
   P(\theta|X) & = \frac{P(X|\theta) P(\theta)}{P(X)} \\[2ex]
               & \propto P(X|\theta) P(\theta) 
\end{aligned}\tag{7}
$$

所以，式 $(6)$ 中关于参数 $\theta$ 的求解对应地变为

$$
\begin{aligned}
   \theta_{MAP} & = \arg \max_{\theta} P(\theta|X) \\[2ex]
                & \propto \arg \max_{\theta} P(X|\theta) P(\theta) 
\end{aligned}\tag{8}
$$

类似地，我们只需对 $P(X\mid \theta) P(\theta)$ 求关于参数 $\theta$ 的偏导并令其为零，即可解得 $\theta_{MAP}$。

我们可以看出，在模型参数的先验分布是均匀分布时（即 $P(\theta)=1$），最大后验估计与极大似然估计是完全等价的。此时称为无信息先验( Non-informative prior )，通俗的说就是“让数据自己说话”，此时贝叶斯方法等同于频率方法。

随着数据的增加，先验的作用越来越弱，数据的作用越来越强，参数的分布会向着最大似然估计靠拢。而且可以证明，最大后验估计的结果是先验和最大似然估计的凸组合。

<br/>

# **总结**

**MLE：** 通过极大化似然函数 $P(X\mid \theta)$ 求解模型参数 $\theta$。

**MAP：** 通过极大化似然函数 $P(X\mid \theta)$ 与模型参数的先验概率 $P(X)$ 的乘积来求解模型参数 $\theta$。

1. 在拥有准确的模型参数的先验概率 $P(X)$ 的情况下，最大后验估计拥有对参数 $\theta$ 更为准确的估计。

   
2. 若模型参数 $\theta$ 的先验分布是均匀分布，即 $P(\theta)=1$。此时最大后验估计与极大似然估计完全等价。


3. 若所拥有的模型参数 $\theta$ 的先验概率 $P(X)$ 不够准确，最大后验估计的效果可能不如极大似然估计。


好滴，今天关于统计知识的小课堂就到这里啦~ 希望能帮到大家！

下期再见！👋
